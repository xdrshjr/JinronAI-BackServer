# MCP聊天机器人项目概述

## 项目简介

MCP聊天机器人是一个演示如何将模型上下文协议（Model Context Protocol，简称MCP）集成到简单命令行界面聊天机器人中的示例项目。该实现通过支持来自MCP服务器的多种工具，展示了MCP的灵活性，并与任何遵循OpenAI API标准的LLM提供商兼容。

## 核心特性

- **LLM提供商灵活性**：适用于任何遵循OpenAI API标准的LLM（已在Groq上的Llama 3.2 90b和GitHub Marketplace上的GPT-4o mini上测试）。
- **动态工具集成**：工具在系统提示中声明，确保在不同LLM之间的最大兼容性。
- **服务器配置**：通过简单的JSON配置文件支持多个MCP服务器，类似于Claude桌面应用程序。
- **工具自动发现**：自动从配置的服务器中发现可用工具。
- **标准化工具执行**：通过MCP协议标准化处理工具执行过程。

## 项目架构

该项目采用模块化设计，主要包含以下几个核心组件：

1. **配置管理（Configuration）**：负责管理环境变量和服务器配置
2. **服务器管理（Server）**：处理MCP服务器初始化、工具发现和执行
3. **工具表示（Tool）**：表示具有属性和格式化功能的各个工具
4. **LLM客户端（LLMClient）**：管理与LLM提供商的通信
5. **聊天会话（ChatSession）**：协调用户、LLM和工具之间的交互

## 工作流程

以下是项目的基本工作流程：

1. **初始化阶段**：
   - 加载环境变量和服务器设置
   - 使用各自的工具初始化服务器
   - 发现工具并为LLM理解格式化

2. **运行时流程**：
   - 接收用户输入
   - 将输入发送给LLM，并提供可用工具的上下文
   - 解析LLM响应：
     - 如果是工具调用 → 执行工具并返回结果
     - 如果是直接响应 → 返回给用户
   - 将工具结果发送回LLM以供解释
   - 向用户呈现最终响应

3. **工具集成**：
   - 从MCP服务器动态发现工具
   - 在系统提示中自动包含工具描述
   - 通过标准化的MCP协议处理工具执行

## 技术要求

- Python 3.10
- `python-dotenv`
- `requests`
- `mcp`
- `uvicorn`

## 文档结构

本教程分为以下几个部分：

1. **项目概述**（当前文档）- 整体介绍项目架构和功能
2. **环境设置与安装指南** - 如何设置和运行项目
3. **配置管理模块详解** - 深入了解配置模块的工作原理  
4. **服务器与工具管理详解** - 服务器管理和工具处理的详细介绍
5. **LLM客户端与聊天会话详解** - LLM通信和用户交互的实现
6. **扩展与自定义指南** - 如何扩展和定制该项目

通过学习这些文档，您将全面了解MCP聊天机器人的工作原理，并能够根据自己的需求进行定制和扩展。 